Ethical considerations in the use of Machine Learning for research and statistics. Machine learning algorithms work by learning from “training” data and applying that learning to new, unseen data. By using machine learning, analysts are able to identify trends and patterns in very large datasets. The information collected from machine learning can be descriptive (it uses data to explain a phenomenon).
predictive (it predicts what will happen based on trends and patterns from the data given).
prescriptive (it can make decision-making suggestions).
This means that machine learning methods have an incredibly wide range of application.
Supervised machine learning In this instance, algorithms are fed labelled data (data which is annotated so that the machine knows its target) for training. Supervised learning leads to a prediction or classification of a known quantity (i.e., an outcome variable), using patterns that the machine finds in the data to predict an outcome. For example, if a data scientist wanted to teach a system to identify cats in different images, they would feed the system with images of cats, labelled as cats. Unsupervised machine learning
Unlike supervised machine learning, in which the machine is fed input variables and an output variable, unsupervised machine learning only uses input data. This means that the model learns without supervision to discover its own patterns and information from the data. This type of machine learning assists us in finding unknown patterns in data. The data fed into the model is typically unlabelled. Using the same example as in the supervised machine learning, in this instance, a data scientist would feed the system with images of cats, but they would not be labelled, and it would be the responsibility of the system to analyse the data and predict which images are of cats. Reinforcement machine learning trains models to make decision sequences, by utilising a process of trial and error. The programmer will reward the machine when it does what the programmer wants and penalise it when it does not (though the programmer will not give the models help in making these decisions). The model then will try to maximise its reward, causing it to change its decisions (strategise). 


Examples of Machine Learning projects in research and statistics include Using satellite data to estimate land coverage and land usage.
Understanding perceptions of public emergencies using social media data to consider impacts and possible mitigations.
Using text data to classify businesses according to their economic activity.
The use of machine learning data in research and statistics provides substantial potential benefits. Particularly beneficial is the ability to analyse large data sets and extract information quickly once a model is deployed. The automation of tasks may be less resource intensive and the ability for models to autonomously adapt to improve the quality and validity of outcomes is often invaluable to data scientists, researchers, and analysts. As such, machine learning is now used in multiple types of research. When analysts embark on a research project using any method, it is always important that any possible ethical issues relating to the collection, access, use and storage of data are considered. This helps reduce potential harm to all individuals involved in the research and helps maintain public acceptability around the production of research and statistics, and enables researchers to efficiently access and harness data that supports the production of statistics for the public good. These ethical issues are particularly important when using more contemporary methods such as machine learning, as it poses not only traditional data ethics considerations such as transparency and privacy concerns, but also new ones. When algorithms are used, it may be more challenging to guard against mistakes or bias, which may result from human interaction with the model (for example, coding, design decisions or data input), and that could affect the system’s outputs. Moreover, research findings may be biased or erroneous should models be used outside of their intended purpose, or if machine outputs are not thoroughly reviewed and checked for validity before use.

Of course, despite these ethical considerations, there are huge benefits to using machine learning methods. Taking a considered approach to ethics in every project helps to maintain public trust in the use of data for research and statistics more generally, enabling researchers to harness the power of data to support public good research. No matter what stage your machine learning project may be at, it is always sensible to discuss possible ethical issues that could arise with other researchers. This applies if you are thinking about starting a new project using machine learning, in the process of designing your study, or even if you have started to create, or deploy, a machine learning system. Of course, it is always beneficial to start thinking about ethical challenges at the earliest possible stage of the research. By doing this you are implementing good data ethics by design.

The UK Statistics Authority provides researchers with an ethics self-assessment tool, which is used to empower researchers to identify and review any ethical challenges apparent in a research project. This guidance supplements the ethics self-assessment tool and also provides a high-level checklist that you can use to ensure that any research or statistical project that uses machine learning techniques is ethically responsible. To help analysts navigate potential ethical issues, the UK Statistics Authority has developed six ethical principles to consider throughout the life cycle of a research project. These principles focus on ensuring the public good of research and statistics, maintaining confidentiality of data, understanding the potential risks and limitations in new research methods and technologies, compliance with legal requirements, considering public acceptability of the project, and transparency in the collection, use and sharing of data. 
This guidance is underpinned by these general principles but focuses specifically on ethical considerations relating to machine learning which require us to take particular care. These include The importance of minimising and mitigating social bias and subsequent discrimination within machine learning research, and clearly communicating these biases, and the limitations of our research.
The need to consider the transparency and explainability of machine learning research, and the implications this has for reproducibility.
The importance of maintaining accountability within all aspects of machine learning processes, ensuring that models are used only for their intended purposes, and that different stakeholders are aware of their responsibilities.
The need to consider the confidentiality and privacy risks arising from the data used, both in relation to training data which is fed into the machine, and outputs resulting from the machine learning’s findings. Also worth reflection here is the Office for Statistical Regulation’s Code of Practice for Statistics. The OSR Code of Practice sets the standards that producers of official statistics should commit to. Specifically, the Code of Practice is framed around three main pillars. These are value, quality, and trustworthiness. These support, and map clearly onto, the UK Statistics Authority’s ethical principles and are important statistical standards. We can benefit greatly from considering these principles in relation to machine learning, and the ethical issues addressed below. 

There are several ways in which bias can be reflected within machine learning which could influence how algorithms are created, used, and interpreted by analysts. Again, whilst there may be some overlapping relevance to operational or other uses of machine learning, this guidance focuses on the implications of different ethical challenges for research and statistics. In many circumstances, bias in machine learning is a result of cognitive human error – problems are introduced by those who design or train the systems. For example, the training data may be incomplete, or unrepresentative of the correct population. Bias though does not always exist as a result of human error, for example, it may be introduced into data sets or models as a consequence of previous societal norms.

The potential biases associated with machine learning are not unique to machine learning methods (for example, sample bias is a typical ethical (and quality) consideration within traditional statistical methods), and mitigations used to limit bias in traditional statistical methods can certainly help when thinking about machine learning. That being said, issues of bias in machine learning may be presented in more complex ways (particularly when using large datasets) and can often be more prominent for machine learning application. Detecting bias within machine learning studies can be very difficult, and understanding and limiting the effects of bias once it is identified can be a complex process.

There are a number of different biases which can be found within machine learning projects. Four of the most common are outlined below.

Sample bias
This occurs when the data used to train a model is not representative of the correct population.

Algorithmic bias
This is the bias that occurs when the algorithm being used is faulty (for example there may be technical issues with the algorithm stopping it from working), or not being used in the way it was intended (and therefore not appropriate for the new application).

Prejudicial bias
This refers to a situation in which the model reflects existing prejudices within the training data. A model will apply the same stereotyping that exists in society, if it is fed prejudicial data, and this will influence its results. Similar to prejudicial bias is observer bias, which relates to the conscious and unconscious prejudices or judgements which a researcher may bring to their data. This may affect both the representativeness of the training data and therefore the results from the system, but also the interpretation of results or recommendations.

Exclusion bias
If a data point or set of features (such as location, geography etc.) is left out of the training data, exclusion bias may occur. This normally happens when a researcher takes a data point out of the training data thinking it is not consequential, but this can lead to inaccuracy in the system.
A common solution to make sure training data doesn’t perpetuate bias from sensitive attributes (such as gender, ethnicity, socio-economic status etc.) is to remove these features from the training data. However, this may not eradicate the bias completely, and can in some instances, worsen the problem, as correlated attributes still existent in the data may still reflect the bias you are trying to erase. For example, a name might be correlated to gender, or a postcode may be correlated to socio-economic status.

If you do decide to discard a sensitive attribute (or indeed, any part of your data), you should document this thoroughly,  investigating and analysing the feature’s relevance to the data before deletion, and the relationship between this feature and your label(s). It may help to ask a colleague for a second opinion.

The Data Science Campus present methods for cleaning faulty data to improve reliability and accuracy in their research on “Estimating vehicle and pedestrian activity from town and city traffic cameras”, which provides an interesting example of machine learning being used for statistical purposes.

Take time to reflect on possible biases that may find their way into the data you are using. Any potential for bias, no matter how big or small, and the impacts that may arise from this should be clearly communicated with all stakeholders and documented throughout the project. Reflexivity on the part of the researcher is key.
It may be useful to discuss your project with colleagues who are not involved in the project as part of an independent review.
Ensure that training datasets are as representative of the correct population as possible. This may help counteract sample and prejudicial bias.
You cannot expect your model to learn what a cat looks like, if you only feed it images of dogs. It is important that all the cases that we might expect a model to be exposed to are examined and ensure that a well-balanced data set is used.
Where possible, the algorithms and data sets should be tested and validated to mitigate any possible biases, and systems should be systematically monitored to ensure that biases do not occur as the systems continue to learn.
Machine learning models will sometimes perform differently in the real world than they did when being trained, and so it is important that models are continuously modelled and audited so that any issues can be identified quickly. Documenting the decisions that are made at all stages can be beneficial in determining where, and how, bias could have been introduced.

One of the UK Statistics Authority’s ethical principles –  transparency – relates to the obligation of researchers to ensure that the decisions they make about their data, analysis, and methods, are openly and honestly documented and communicated in a way that allows others to evaluate them. Explainability then can be used to define machine learning techniques that as humans, we are adequately able to understand, trust and manage. Though not synonymous, issues of transparency and explainability are inextricably linked with one another, and so for the purpose of this guidance, the two challenges will be addressed together.

These concepts relate to the potentially opaque nature of machine learning algorithms, and the difficulty in communicating how these systems are used and how they work, particularly to a lay audience. So-called “black-box algorithms” (where the researcher knows what data is fed into the algorithm, and what comes out, but might not be able to interpret it in actionable, human terms) are central to considering this issue. There may be good reasons for using a black box approach – for example, a black box algorithm may provide better quality or more accurate data than a more transparent alternative – but it is important to note, however, that there are explainable machine learning algorithms, which should be considered to optimise transparency, where it is appropriate to do so.  This will depend on a number of factors, including the intended use of the model, the maintainability of the model and its outputs, and whether using the more transparent alternative risks sacrificing the quality or accuracy of another, more opaque model. Of course, there are far greater complexities when it comes to algorithmic explainability than allow us to make binary distinctions between “black box” algorithms” and “explainable” algorithms, and there a number of “model-agnostic” methods which may help researchers (in some instances, and with certain caveats) interpret machine learning models. Explainability then is not black and white, and should be seen more as a sliding scale.

A lack of transparency in machine learning projects has further implications for the reproducibility of results. The ability to replicate results is crucial in ensuring and maintaining scientific trust. Being transparent about the way in which data is used, and how our research is carried out allows others to verify the results of a project via replication, and the accuracy (or inaccuracy) of a model, enabling researchers to determine whether the model is sufficient, and whether it is suitable for further use. Transparency is particularly important to consider when using technologies such as machine learning, especially with the aim of reproducibility in mind. For instance, the researchers deploying systems may not be those who made that system to begin with, which makes validating the quality of the system and accurately predicting the decision-making rationale of the person who made the system much harder. Regular and systematic documentation of all coding processes and decisions made are therefore the building blocks of transparency.

There are many different stakeholders that need to be considered when thinking about transparency, and communicating your research, from members of the public to internal and external analysts. In this guidance we refer to all those interacting with machine learning as stakeholders, however it is important for researchers to consider who these different stakeholders are, and tailor communication with each group for best impact. If in doubt, assume that the group that you are interacting with has no experience of machine-learning – it is better to simplify your language to ensure that stakeholders understand your message.

It is also worth considering how machine learning projects can benefit us, and their limitations. For example, whilst machine learning algorithms can help us make predictions, most are not causal. This means that conclusions can rarely be drawn that are beyond correlation, therefore it is often not possible to make claims of cause and effect.

It is important for all stakeholders to understand that they are interacting with machine learning, and the conclusions and recommendations that come from a machine learning model. If stakeholders, from participants to analysts, are not given clear information to help them understand the research, then this risks promoting harmful practices. Explainability and transparency are important considerations for researchers to build public confidence and promote ethical practice.

Everyone interacting with the project should be able and encouraged to ask questions at any point within the research process, and this should be communicated in a user interface, or similar document. These questions should be answered in a transparent and timely manner.
If the choice is taken that an opaque machine learning algorithm is to be used over a more explainable one, researchers may need to consider how this can be communicated to a non-specialist audience. In these cases, consideration should be given as to how information can be communicated transparently and in ways that are easy to understand to relevant non-expert communities as necessary. Researchers using machine learning techniques should communicate in plain language why the opaque algorithm was chosen, and what its advantages are that make it the best choice despite the lack of explainability, as well as what training data has been used, any bias or limitations of the data, and whether the training data has been validated through other studies. Considerations of the quality of the training data may also provide a useful framework for communicating some of these issues, for which existing guidance may be helpful.
Researchers might also find it useful to produce a clear statement of why their conclusions are believed to be valid, what is meant by ‘being valid’ for their particular use case, and any limitations on the validity.
All human decision-making processes should be audited and easily reviewable. It may be useful to systematically review each decision-making process at different stages of the research, clearly documenting the rationale and impact of these decisions. Creating an audit trail of human decision-making throughout a machine learning process supports analysts in explaining to non-expert audiences how and why machine learning has been used, and any potential ethical implications of this use.
It may be useful to discuss your project with independent colleagues as part of “challenge sessions”, both to maximise collective understanding and to provide an additional opportunity to check the model is doing what is expected.
An explanation of the outputs and any associated recommendations arising from the project should be easily accessible and as understandable as possible, supporting non-expert understanding of the impact of the research findings. This should be delivered in a timely manner.
It is always helpful to put yourself in the shoes of other stakeholders when you design any project, and this can be particularly helpful when thinking about how to communicate complex ideas (such as machine learning algorithms and systems) to non-expert audiences. The exercise below may help analysts think about these concepts and communicate with users effectively.

Communicating with non-experts when using machine learning techniques to ensure transparency.

What information might your audience find helpful?

Take a minute to consider what you would like to know about a project if you were approached by someone who wanted to tell you about their machine learning research. Consider this from the perspective of the audience you are trying to communicate with.

What information might your audience not find helpful?

Take a minute to consider what information you may not find helpful if you were approached by someone who wanted to tell you about their machine learning research from the perspective of the audience you are trying to communicate with.

Helpful
What is machine learning?
Why did you choose to use machine learning over other methods?
What is the aim of the research?
Why is the study important, and what will you do with the results?
Were there any limitations to your research, or the machine learning methods that you used?
How did you access the data and how will it be used?
Why did you choose to use this dataset?
Will the outcome of the research affect groups or individuals (either positively or negatively)?
Is my personal information safe? How is my data being stored and can it be reused for other purposes?
Will anyone be identifiable from the data or the research outcomes?
Not Helpful
What will the typical lay person learn from being presented with an algorithm? Are they likely to understand it, or is there a more understandable way of presenting this to a lay audience?
Too much information! It may put users off if there is too much information, or if the information presented is hard to read. How can you present the information in a way that is concise and easy to read?
Helpful
What is the key message in regard to policy? What was the key policy challenge that the research seeks to address?
Why is this research important for policy? What are its main implications?
Are the policy implications of the research short term or long term?
What are the key findings from the research?
How have you arrived at these findings? What methods were used?
What will happen with the data collected, and how will it be used going forward?
Are there any limitations/assumptions to the research?
What are the key policy recommendations?
It may also be beneficial to consider the views of key stakeholders, particularly in relation to the public. For example, if a policy decision was made on the basis of your project, would the public be comfortable with this?
Not Helpful
Much like the public audience, policy and decision-makers are unlikely to gain from being presented with the inner-workings of your algorithm, or the granular specifics of your methods. Instead, a brief, high-level summary of how you have come to your findings may be more useful.
Policy and decision-makers are likely to be limited on time, and so it is critical to keep communication with them succinct. How can you present the information in a way that is concise and easy to read or visualise?
How can you present your research findings in a way that is relevant to policy, and which clearly highlights its aims, its importance, its limitations, and its implications?
Helpful
What training data was used to teach the system?
How did you obtain and quality assure the data that you used to teach the system?
How did you train the system (which methods did you use throughout the process)?
Were there any limitations or biases in the training data that may have affected the results? How have these been mitigated?
What patterns or recommendations have emerged from the data? How did the machine learning model come to this conclusion?
What are the assumptions related to this recommendation?
How much better is this approach than others that could have been used? Are there any improvements that could be made to the current approach?
How was the model evaluated/compared against other models?
Are there plans in place to continue the work with updated data?
Have you shared your code for others to use and adapt?
Not Helpful
This group may have a more technical understanding of machine learning, or statistical processes, and so more detailed information regarding how the system was trained, and how it reached the results it did could be useful.
Think about what you have been asked to do and why, and what the benefits of this are. You can then tailor your communication with this in mind.
These questions have been designed as a starting point for conversations with different key groups, and should provide a good starting point when thinking about how to communicate with different audiences.
Publishing your algorithms for people to see is really useful and goes some way to ensuring transparency. However, many people may not be able to understand or interpret an algorithm. Making your algorithm accessible is not enough to make your research transparent! Perhaps you could consider linking your audiences to the published algorithm should they want to see it but consider the questions in the left-hand column to better explain what it means to your audience.

You will need to communicate your research to different audiences, and they will likely have varying levels of understanding. It is important to tailor your communications with each group to ensure transparency, however, by considering the questions above in relation to a non-expert user, you may find it easier to communicate this information to all stakeholders.

By placing ourselves in the shoes of a person with no knowledge of machine learning (or by practicing explaining our work to people with no knowledge of it), not only are we able to better communicate with this group of users, but we can also
begin to think more transparently about our work.
better communicate with a lay audience.
ensure explainability.
better understand our own research.
improve the impact of our work on different audiences.
Though there are many complex moral and legal issues surrounding accountability, every person who is involved in the creation of a machine learning system has a degree of responsibility for considering the system’s ramifications, good or bad (and researchers will need to be aware of their organisation’s own different approaches and standards). It is not the responsibility of the machine or system to be considerate of these ideals. This is the responsibility of those who establish these systems, and they should be accepting of scrutiny and appropriate accountability.

Accountability is the willingness to accept responsibility for a process, and therefore be answerable to any query or issue that may arise as a result.  Organisations should agree on appropriate mechanisms to ensure accountability in-line with accepted standards. The Office for Statistical Regulation have produced guidance on good practice in designing, developing, and using models, which may be useful in helping to think about these mechanisms.Ensuring the appropriate use of machine learning, and machine learning models. 
When designing a research project it is imperative that the benefits and challenges of different methods are weighed up, and that the researchers are able to justify the method(s) that they have chosen to use. This should be clearly documented.

In the case of machine learning, it is also the collective responsibility of those who develop and train models, and those who deploy pre-existing models to ensure that the use case of a model is clear, and that models are not used beyond their intended use.

Training a machine learning model can take anywhere between hours and months, depending on the problem you are trying to solve, and the data being used. Using a pre-existing model with the same intended use as a basis for your project may save time and money. In addition, training machine learning models also has an environmental impact, which could be lessened by using pre-existing models.  However, in order to ensure transparency and accountability, it is important to consider how the model being used was developed, and for what purpose. Utilising models beyond their intended use without care and caution may impact upon the accuracy and validity of your results. Using pre-trained models may be particularly problematic if the model lacks transparency, as it may be harder to identify the processes used to train the model, and existing biases.

Whilst developers cannot stop others from using their models, to remain accountable, anyone making and training models should try to be explicit in communicating the intended use of a model, so that models are not created that are then used by others in the wrong way. If you are considering using a pre-trained model, it is important to understand what the model was built to be used for, and ensure before it is implemented, that it is suitable for your own project. 
If you play any role no matter how big or how small in the design or development of a machine learning project, it is your responsibility to understand what your role is, and the policies relevant to your role. You should be accepting of scrutiny – this could improve your research, help you with your decision-making process, and ensure a level of public trust.
Ensure accountability by design, by having governance processes to ensure human oversight at the appropriate organisational level throughout design and implementation.
It is vital that a model is audited at regular intervals, and that this is well documented. Sufficient time should be built into the project plan to allow for this. This will enable assurance that the machine learning technique fulfils the intended purpose without any unwanted consequences. Regular audit allows scope drift and risk to be identified, mitigated against, and documented as required by the relevant stakeholders.
A continuous chain of human responsibility should be established through the full lifecycle of the machine learning model. All human involvement and oversight should be transparent, documenting all roles and actions taken to ensure human responsibility can be identified. Machine learning – particularly when using predictive analytics – often involves linking data from multiple sources. New data sources (such as social media data and biometric data) are increasingly being used by analysts to identify social trends, and using these sources raises ethical questions surrounding confidentiality. Put simply, confidentiality refers to the measures taken to ensure that data and data subjects are protected from being identified, via the separation or modification of personal information provided by participants from the data.

There are many ways in which we can protect confidentiality, including techniques such as the anonymisation of datasets, de-identification, and data management and security processes (such as limiting access to data to specific, agreed purposes). The techniques used will depend on several factors, including the type of data being used, and for what purpose. In choosing which methods may be most appropriate for your project, it may be helpful for researchers to consider the 5 Safes Framework, which has been adopted by the  Office for National Statistics’. This helps researchers maximise the use of their data, whilst ensuring that the data is kept secure at all times.
Potential Mitigations
Once the dataset has been explored, and the data necessary for the research has been determined, additional data should be deleted. However, this should be done with care, and only after careful consideration, as deletion of some data may make the model worse, or less transparent for the user.
If anonymising data, this should be done at the earliest opportunity and consideration should be taken to ensure that the most appropriate method(s) of anonymisation are used (e.g., natural language processing- based anonymisation, differential privacy, data masking, aggregations). Ensuring the exclusion of unnecessary data will help prevent the system from identifying unhelpful correlations.
Consideration should be given to the level of control data subjects should have over their own data, balancing the risks to individual privacy against the statistical value of the dataset. Data subject rights can be specified in materials that are publicly available to data subjects (such as privacy notices or participant information) and organisations should have appropriate mechanisms to delete or withdraw data as required.
Has the project been considered in relation to the UK Statistics Authority’s general ethical principles?
Public good: Have the benefits of using machine learning for a project been clearly documented. For further guidance on communicating public good, see our Public Good guidance.
Methods and quality: Is machine learning the most suitable method to use? Have the limitations of machine learning data/technologies been considered?
Transparency: Has transparency in the collection, use, retention and sharing of the data being used and produced been considered?
Legal compliance: Has relevant regulation been considered in relation to the dataset used, both in the UK and if necessary, overseas?
Public views and engagement: Have potential public views regarding particular uses of machine learning data across different contexts been considered?
Confidentiality and data security: Have appropriate mechanisms to maintain confidentiality of datasets been considered?
Have you considered the potential for bias, which could arise from any of your data?
Have you scrutinised your training data for potential biases, and considered the potential for your own conscious or sub-conscious biases to be reflected in the data?
If potential bias is identified, ensure that this is documented to enable informed interpretation of results. What are the potential implications of this bias, and how can this be minimised?
Who are the key stakeholders who need to be considered when communicating your project, and what are likely to be their main questions and concerns? Assuming that you had no knowledge of the project, what would you like to know about how your data is being used to provide outputs that inform decision-makers?
Are you able to explain what data is being used to train the algorithm, and what you expect to get from the data afterwards?
Have you enabled an open and transparent system to allow stakeholders to ask questions throughout the research process?
Would another researcher be able to reproduce your results with the information available to them?
Has human accountability been built into the project from the design phase? Are there structures in place to enable accountability?
Has a chain of human responsibility been established, with each stage of the project’s lifecycle being documented to show the human oversight?
Has time been put aside, throughout the lifecycle, to account for an audit of the machine learning model?
If you have created a model yourself, has the intended use of the model been clearly communicated to ensure that it is not misused?
If you are using a pre-existing model, have you ensured that you are using it in the way it was intended?
Has data minimisation been appropriately considered? Only the data that is required should be stored and used, and any unnecessary data should be deleted once it has been determined that it is appropriate to do so.
Have you considered whether it is appropriate to anonymise your data, and if so, what the most appropriate method(s) of anonymisation will be?
Have you ensured that your data is being safely stored?
Might your data, system, or results be re-used outside of their original context and purpose in the future to the disadvantage of individuals, groups or communities? What can you do to try and protect against this possibility?

